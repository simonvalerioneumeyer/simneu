[
  {
    "slug": "paris-lcv-stop-location-modeling",
    "title": "French LCV Traffic Modeling (Paris)",
    "summary": "Built a two-part ML pipeline for Paris light commercial vehicle analysis: (1) stop-location prediction and (2) stop-duration prediction.",
    "technologies": [
      "Python",
      "Pandas",
      "GeoPandas",
      "NumPy",
      "Polars",
      "scikit-learn",
      "XGBoost (classification and regression)",
      "Parquet-based geospatial data pipelines"
    ],
    "problem": "Urban freight analysis required more than origin-destination aggregates. The core need was to estimate where LCVs are likely to stop and how long they remain there, using noisy, large-scale mobility traces enriched with land-use and economic context.",
    "approach": "1) Stop location prediction\n- Prepared LCV trip start/end flows and mapped them to IRIS zones.\n- Built POI candidate sets using KNN + zone-based expansion.\n- Engineered temporal/spatial/context features (hour/weekday, road type/direction/modes, POI category context, zone-level POI and job frequencies).\n- Trained an XGBoost model with hard/soft labeling and weighted training.\n\n2) Stop duration prediction\n- Built a duration feature table from large stop-event data (over 1.3M rows).\n- Added nearest-POI context, IRIS logistics features, and employment/POI zone features.\n- Created train/valid/test splits and trained an XGBoost regressor for stop duration.\n\nData foundation across both tasks: LCV flow/stop data, POI datasets, IRIS boundaries, road-network attributes, and employment/business activity aggregates.",
    "results": "Delivered an end-to-end modeling workflow that produced stop-level location probabilities and duration estimates, plus reusable feature pipelines and model artifacts for repeatable urban freight and mobility analysis."
  },
  {
    "slug": "lagos-public-transit-analytics",
    "title": "Public Transit Experiment Analytics (Lagos, Nigeria)",
    "summary": "Built daily analytics pipelines for an RCT using public transport e-ticketing API data and geospatial analysis for field and evaluation design.",
    "technologies": ["Python", "API Data Pipelines", "Geospatial Analytics", "Impact Evaluation Analytics"],
    "problem": "The project required reliable daily tracking of public transport usage for an RCT, plus robust spatial preparation of supporting datasets for research operations.",
    "approach": "I used a public transport e-ticketing API to scrape and process daily transport records, then produced recurring usage and performance metrics for the RCT. I also supported the pricing workstream by helping design and analyze the public transport ticket-pricing experiment. In parallel, I built geospatial datasets for research design, including optimal sampling areas around bus stops stratified by population and income indicators, and delivered additional technical analyses in Python for the broader study.",
    "results": "Supported day-to-day RCT monitoring with reproducible Python analytics and provided analysis inputs to the broader research effort, which was later published by NBER (I supported analysis but am not a co-author).",
    "publicationUrl": "https://www.nber.org/papers/w33899"
  },
  {
    "slug": "global-mobility-decarbonization",
    "title": "Global Decarbonization Mobility Analytics",
    "summary": "Built and scaled a global mobility measurement pipeline on over 120TB of geospatial ping data to estimate daily travel distances across more than 150 countries.",
    "technologies": ["Python", "Pandas", "Dask", "GeoPandas", "NumPy", "AWS S3", "PyArrow/Parquet", "Large-Scale Geospatial Analytics"],
    "problem": "The project needed globally comparable, policy-relevant mobility metrics from highly noisy device-level pings, while preserving consistency across countries with very different data coverage and quality.",
    "approach": "I developed an end-to-end country/day processing workflow that ingested raw mobility pings from S3, mapped devices to urban areas, computed distance and speed features at segment level, and produced device-level daily distance metrics before urban-area aggregation. The pipeline applied explicit QA and harmonization rules, including daily coverage thresholds (e.g., 3-hour block filters), speed and distance outlier filtering, and traverser/non-unique movement handling. I then aggregated to urban-area/date/country indicators (mean/median distance and device counts), ran cross-country diagnostics and device-level descriptives, and implemented logistic-growth-based adjustment using observed coverage (5-minute segments) to correct for sparse sampling and improve comparability.",
    "results": "Delivered reproducible mobility indicators and comparison tables/maps used for decarbonization analysis, with documented sensitivity checks (coverage, traverser removal, and filter configurations) to support robust interpretation across countries. Outputs also included a dashboard used by different country offices to support policy decisions, and a related paper that is about to be published."
  },
  {
    "slug": "addis-road-safety-cv",
    "title": "Road Safety Impact Evaluation (Addis Ababa)",
    "summary": "Built a full video-to-metrics pipeline for road safety impact evaluation, from object detection/tracking in intersection footage to conflict/traffic analytics and reporting.",
    "technologies": [
      "Python",
      "YOLOv7",
      "DeepSORT",
      "Computer Vision",
      "Geo/Spatial Data Processing",
      "AWS S3",
      "Pandas/Parquet Analytics"
    ],
    "problem": "Manual video review could not scale across many intersections and time periods, making it difficult to produce consistent conflict and traffic indicators for baseline/follow-up evaluation.",
    "approach": "I implemented and operationalized a two-stage workflow. Stage 1 (video processing): ran YOLOv7 + DeepSORT on MP4 intersection footage to detect and track road users, and produced annotated outputs plus metadata (JSON/parquet), with batch execution and storage to S3. Stage 2 (data processing): transformed per-location parquet outputs into analysis-ready datasets using movement polygons, direction splits, geolocation/homography metadata, and baseline/follow-up logic; generated conflict and traffic aggregates, trajectory plots, short reports, and one-pager/dashboard-ready artifacts. I also supported subsampling/bootstrapping analyses to evaluate metric stability when using partial data.",
    "results": "Delivered reproducible, scalable safety analytics across locations, reduced manual processing load, and enabled longitudinal traffic/conflict comparisons for impact evaluation reporting. The related report is currently under review and expected to be published in 2026. The codebase is also being packaged for reuse in other World Bank projects."
  },
  {
    "slug": "kosovo-credit-impact-ml",
    "title": "Credit Impact Evaluation (Kosovo)",
    "summary": "Used ML and statistical analysis to identify drivers of credit access and predict potential MSME borrowers.",
    "technologies": ["Python", "Statistical Modeling", "Machine Learning", "Feature Engineering"],
    "problem": "The evaluation needed a data-driven framework to better understand borrower access barriers and target eligible firms.",
    "approach": "Engineered borrower-level features, trained predictive models, and combined model outputs with interpretable statistical diagnostics.",
    "results": "Improved identification of potential MSME borrowers and strengthened evidence for financial inclusion strategies."
  },
  {
    "slug": "enterprise-cost-prediction-novartis",
    "title": "Cost Prediction Model Refinement (Novartis)",
    "summary": "Rewrote and refined internal cost-prediction models to improve robustness, interpretability, and business alignment.",
    "technologies": ["Python", "Forecasting", "Model Validation", "Business Analytics"],
    "problem": "Legacy internal models needed improved stability and clearer linkage to business performance metrics.",
    "approach": "Refactored model logic, improved feature handling and validation, and aligned evaluation metrics with stakeholder goals.",
    "results": "Delivered more reliable and interpretable model outputs for operational planning."
  }
]
